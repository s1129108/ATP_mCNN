{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271c45cf-e40c-417c-a83e-03b375add143",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbd76d2-6c8b-4901-896e-85c3a298082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 22:43:32.098692: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 22:43:32.157200: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-16 22:43:32.451588: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-16 22:43:32.451630: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-16 22:43:32.451633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model\n",
    "\n",
    "##\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#import import_test_ETC as load_data\n",
    "import import_test_atp as load_data\n",
    "#import import_test_nine as load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e9da3-3fb9-4ed6-a197-380544b91cf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PARAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0851d5dd-21d4-4182-8fa5-2027de0091de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABEL=load_data.data_label()\n",
    "\n",
    "\n",
    "NUM_DEPENDENT =7\n",
    "MAXSEQ = NUM_DEPENDENT*2+1\n",
    "\n",
    "DATA_TYPE = \"ProtTrans\"\n",
    "#\"/BinaryMatrix\" \"/MMseqs2\" \"/ProtTrans\"\n",
    "\n",
    "\n",
    "NUM_FILTER = 256\n",
    "NUM_HIDDEN = 1000#100\n",
    "BATCH_SIZE  = 1024\n",
    "WINDOW_SIZES = [4,6,8,10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "CLASS_NAMES = ['Negative','Positive']\n",
    "\n",
    "NUM_FEATURE = 1024\n",
    "EPOCHS      = 20\n",
    "\n",
    "K_Fold = 5\n",
    "VALIDATION_MODE=\"independent\"\n",
    "#\"independent\" \"cross\"\n",
    "DATASET=\"new388+41\"\n",
    "#\"new388+41\" \"old227+17\"\n",
    "IMBALANCE=\"None\"\n",
    "#None\n",
    "#SMOTE\n",
    "#ADASYN\n",
    "#RANDOM\n",
    "\n",
    "\n",
    "\n",
    "#IMBLANCE=\"RANDOM\"#\"ADASYN\" \"RANDOM\" \"None\" \"SMOTE\"\n",
    "#SHUFFLE=\"Non-SHUFFLE\"#Non-SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f97ec2-32e3-4f56-af7d-2199b0460b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "write_data=[]\n",
    "a=datetime.datetime.now()\n",
    "write_data.append(time.ctime())\n",
    "write_data.append(DATA_LABEL)\n",
    "write_data.append(DATASET)\n",
    "write_data.append(DATA_TYPE)\n",
    "write_data.append(WINDOW_SIZES)\n",
    "write_data.append(NUM_FILTER)\n",
    "write_data.append(NUM_DEPENDENT)\n",
    "write_data.append(VALIDATION_MODE)\n",
    "write_data.append(IMBALANCE)\n",
    "\n",
    "#write_data.append(IMBLANCE)\n",
    "#write_data.append(SHUFFLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509be09-f6b8-4bc1-8015-97d7de457ff0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d68ed81-93c3-45ac-83e8-db22fde90d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_log(message):\n",
    "    print(message,\" : \",strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad8130-4a7e-45ae-b873-d21cf32fd367",
   "metadata": {},
   "source": [
    "# MCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830c0e76-2144-4ea4-a3d3-c66e0c563930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepScan(Model):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t             input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "\t             window_sizes=[1024],\n",
    "\t             num_filters=256,\n",
    "\t             num_hidden=1000):\n",
    "\t\tsuper(DeepScan, self).__init__()\n",
    "\t\t# Add input layer\n",
    "\t\tself.input_layer = tf.keras.Input(input_shape)\n",
    "\t\tself.window_sizes = window_sizes\n",
    "\t\tself.conv2d = []\n",
    "\t\tself.maxpool = []\n",
    "\t\tself.flatten = []\n",
    "\t\tfor window_size in self.window_sizes:\n",
    "\t\t\tself.conv2d.append(\n",
    "\t\t\t layers.Conv2D(filters=num_filters,\n",
    "\t\t\t               kernel_size=(1, window_size),\n",
    "\t\t\t               activation=tf.nn.relu,\n",
    "\t\t\t               padding='valid',\n",
    "\t\t\t               bias_initializer=tf.constant_initializer(0.1),\n",
    "\t\t\t               kernel_initializer=tf.keras.initializers.GlorotUniform()))\n",
    "\t\t\tself.maxpool.append(\n",
    "\t\t\t layers.MaxPooling2D(pool_size=(1, MAXSEQ - window_size + 1),\n",
    "\t\t\t                     strides=(1, MAXSEQ),\n",
    "\t\t\t                     padding='valid'))\n",
    "\t\t\tself.flatten.append(layers.Flatten())\n",
    "\t\tself.dropout = layers.Dropout(rate=0.7)\n",
    "\t\tself.fc1 = layers.Dense(\n",
    "\t\t num_hidden,\n",
    "\t\t activation=tf.nn.relu,\n",
    "\t\t bias_initializer=tf.constant_initializer(0.1),\n",
    "\t\t kernel_initializer=tf.keras.initializers.GlorotUniform())\n",
    "\t\tself.fc2 = layers.Dense(NUM_CLASSES,\n",
    "\t\t                        activation='softmax',\n",
    "\t\t                        kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "\t\t# Get output layer with `call` method\n",
    "\t\tself.out = self.call(self.input_layer)\n",
    "\n",
    "\tdef call(self, x, training=False):\n",
    "\t\t_x = []\n",
    "\t\tfor i in range(len(self.window_sizes)):\n",
    "\t\t\tx_conv = self.conv2d[i](x)\n",
    "\t\t\tx_maxp = self.maxpool[i](x_conv)\n",
    "\t\t\tx_flat = self.flatten[i](x_maxp)\n",
    "\t\t\t_x.append(x_flat)\n",
    "\n",
    "\t\tx = tf.concat(_x, 1)\n",
    "\t\tx = self.dropout(x, training=training)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.fc2(x)  #Best Threshold\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd6e30-ad98-4e37-a76e-92cd7a39cfc6",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767da2b2-407b-45ab-8840-8056249aa086",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/D/jupyter/atp_binding/dataset/Series15/ProtTrans/new388+41/atp-388'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_train, y_train,x_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMCNN_data_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_DEPENDENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/D/jupyter/atp_binding/code/import_test_atp.py:22\u001b[0m, in \u001b[0;36mMCNN_data_load\u001b[0;34m(DATA_TYPE, NUM_CLASSES, DATASET, NUMDEPENDENT)\u001b[0m\n\u001b[1;32m     19\u001b[0m path_y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/D/jupyter/atp_binding/data/FASTA/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mDATASET\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtraining\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/label\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(path_x_train)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(path_y_train)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m x_train,y_train\u001b[38;5;241m=\u001b[39m\u001b[43mdata_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_x_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m path_x_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/D/jupyter/atp_binding/dataset/Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(MAXSEQ)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mDATA_TYPE\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mDATASET\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtesting\n\u001b[1;32m     24\u001b[0m path_y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/D/jupyter/atp_binding/data/FASTA/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mDATASET\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtesting\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/label\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/D/jupyter/atp_binding/code/import_test_atp.py:35\u001b[0m, in \u001b[0;36mdata_load\u001b[0;34m(x_folder, y_folder, NUM_CLASSES)\u001b[0m\n\u001b[1;32m     32\u001b[0m x_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m y_train \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 35\u001b[0m x_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_folder\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.set.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Iterate through x_folder with tqdm\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(x_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/D/jupyter/atp_binding/dataset/Series15/ProtTrans/new388+41/atp-388'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "x_train, y_train,x_test, y_test = load_data.MCNN_data_load(DATA_TYPE,NUM_CLASSES,DATASET,NUM_DEPENDENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f495f-a64c-40e5-bbf2-fd8874023924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8921de5-8606-439c-88d4-edcb66f74768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMBALANCE_HANDLING(IM_BALANCE,x_train,y_train):\n",
    "    from imblearn.over_sampling import SMOTE,ADASYN,RandomOverSampler\n",
    "\n",
    "    # 將 x_train 的形狀重新整形為二維\n",
    "    x_train_2d = x_train.reshape(x_train.shape[0], -1)\n",
    "    print(x_train_2d.shape)\n",
    "    print(y_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    # 創建 SMOTE 物件\n",
    "    if IM_BALANCE==\"SMOTE\":\n",
    "        imbalance = SMOTE(random_state=42)\n",
    "    elif IM_BALANCE==\"ADASYN\":\n",
    "        imbalance = ADASYN(random_state=42)\n",
    "    elif IM_BALANCE==\"RANDOM\":\n",
    "        imbalance = RandomOverSampler(random_state=42)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "\n",
    "    # 使用 fit_resample 進行過採樣\n",
    "    x_train_resampled, y_train_resampled = imbalance.fit_resample(x_train_2d, y_train)\n",
    "\n",
    "    # 將 x_train_resampled 的形狀恢復為四維\n",
    "    x_train_resampled = x_train_resampled.reshape(x_train_resampled.shape[0], 1, MAXSEQ, NUM_FEATURE)\n",
    "\n",
    "    print(x_train_resampled.shape)\n",
    "    print(y_train_resampled.shape)\n",
    "\n",
    "    x_train=x_train_resampled\n",
    "    y_train=y_train_resampled\n",
    "    \n",
    "    del x_train_resampled\n",
    "    del y_train_resampled\n",
    "    del x_train_2d\n",
    "    gc.collect()\n",
    "\n",
    "    import tensorflow as tf\n",
    "    y_train = tf.keras.utils.to_categorical(y_train,NUM_CLASSES)\n",
    "\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d0239-d249-4956-8deb-680fd3f902fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f0a3d-be25-4106-a4b1-a4bee9f1aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, x_test, y_test):\n",
    "\n",
    "    print(x_test.shape)\n",
    "    pred_test = model.predict(x_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test[:,1], pred_test[:, 1])\n",
    "    AUC = metrics.auc(fpr, tpr)\n",
    "    #tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=AUC, estimator_name='mCNN')\n",
    "    display.plot()\n",
    "    \n",
    "\n",
    "    # calculate the g-mean for each threshold\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    # locate the index of the largest g-mean\n",
    "    ix = np.argmax(gmeans)\n",
    "    print(f'Best Threshold={thresholds[ix]}, G-Mean={gmeans[ix]}')\n",
    "    threshold = thresholds[ix]\n",
    "\n",
    "    y_pred = (pred_test[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    TN, FP, FN, TP =  metrics.confusion_matrix(y_test[0:][:,1], y_pred).ravel()\n",
    "\n",
    "    Sens = TP/(TP+FN) if TP+FN > 0 else 0.0\n",
    "    Spec = TN/(FP+TN) if FP+TN > 0 else 0.0\n",
    "    Acc = (TP+TN)/(TP+FP+TN+FN)\n",
    "    MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)) if TP+FP > 0 and FP+TN > 0 and TP+FN and TN+FN else 0.0\n",
    "    F1 = 2*TP/(2*TP+FP+FN)\n",
    "    print(f'TP={TP}, FP={FP}, TN={TN}, FN={FN}, Sens={Sens:.4f}, Spec={Spec:.4f}, Acc={Acc:.4f}, MCC={MCC:.4f}, AUC={AUC:.4f}\\n')\n",
    "    \n",
    "    return TP,FP,TN,FN,Sens,Spec,Acc,MCC,AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314bd9a-10f4-40ef-a0b1-2d3788a6ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE==\"cross\"):\n",
    "\ttime_log(\"Start cross\")\n",
    "\t\n",
    "\tkfold = KFold(n_splits = K_Fold, shuffle = True, random_state = 2)\n",
    "\tresults=[]\n",
    "\ti=1\n",
    "\tfor train_index, test_index in kfold.split(x_train):\n",
    "\t\tprint(i,\"/\",K_Fold,'\\n')\n",
    "\t\t# 取得訓練和測試數據\n",
    "\t\tX_train, X_test = x_train[train_index], x_train[test_index]\n",
    "\t\tY_train, Y_test = y_train[train_index], y_train[test_index]\n",
    "\t\tif IMBALANCE!=\"None\":\n",
    "\t\t\t\tX_train,Y_train=IMBALANCE_HANDLING(IMBALANCE,X_train,Y_train)\n",
    "\t\tprint(X_train.shape)\n",
    "\t\tprint(X_test.shape)\n",
    "\t\tprint(Y_train.shape)\n",
    "\t\tprint(Y_test.shape)\n",
    "\t\t# 重新建模\n",
    "\t\tmodel = DeepScan(\n",
    "\t\tnum_filters=NUM_FILTER,\n",
    "\t\t\tnum_hidden=NUM_HIDDEN,\n",
    "\t\t\twindow_sizes=WINDOW_SIZES)\n",
    "\t\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\t\tmodel.build(input_shape=X_train.shape)\n",
    "\t\t# 在測試數據上評估模型\n",
    "\t\thistory=model.fit(\n",
    "\t\t\tX_train,\n",
    "\t\t\tY_train,\n",
    "\t\t\tbatch_size=BATCH_SIZE,\n",
    "\t\t\tepochs=EPOCHS,\n",
    "\t\t\tcallbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)],\n",
    "\t\t\tverbose=1,\n",
    "\t\t\tshuffle=True\n",
    "\t\t)\n",
    "\t\tTP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC = model_test(model, X_test, Y_test)\n",
    "\t\tresults.append([TP, FP, TN, FN, Sens, Spec, Acc, MCC, AUC])\n",
    "\t\ti+=1\n",
    "\t\t\n",
    "\t\tdel X_train\n",
    "\t\tdel X_test\n",
    "\t\tdel Y_train\n",
    "\t\tdel Y_test\n",
    "\t\tgc.collect()\n",
    "\t\t\n",
    "\tmean_results = np.mean(results, axis=0)\n",
    "\tprint(f'TP={mean_results[0]:.4}, FP={mean_results[1]:.4}, TN={mean_results[2]:.4}, FN={mean_results[3]:.4}, Sens={mean_results[4]:.4}, Spec={mean_results[5]:.4}, Acc={mean_results[6]:.4}, MCC={mean_results[7]:.4}, AUC={mean_results[8]:.4}\\n')\n",
    "\twrite_data.append(mean_results[0])\n",
    "\twrite_data.append(mean_results[1])\n",
    "\twrite_data.append(mean_results[2])\n",
    "\twrite_data.append(mean_results[3])\n",
    "\twrite_data.append(mean_results[4])\n",
    "\twrite_data.append(mean_results[5])\n",
    "\twrite_data.append(mean_results[6])\n",
    "\twrite_data.append(mean_results[7])\n",
    "\twrite_data.append(mean_results[8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa9cbe-3dc7-41a0-af37-8b2baffd372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(VALIDATION_MODE==\"independent\"):\n",
    "\tif IMBALANCE!=\"None\":\n",
    "\t\tx_train,y_train=IMBALANCE_HANDLING(IMBALANCE,x_train,y_train)\n",
    "    #\n",
    "\ttime_log(\"Start Model Train\")\n",
    "\tmodel = DeepScan(\n",
    "\t\tnum_filters=NUM_FILTER,\n",
    "\t\tnum_hidden=NUM_HIDDEN,\n",
    "\t\twindow_sizes=WINDOW_SIZES)\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\tmodel.build(input_shape=x_train.shape)\n",
    "\tmodel.summary()\n",
    "\n",
    "\tmodel.fit(\n",
    "\t\tx_train,\n",
    "\t\ty_train,\n",
    "\t\tbatch_size=BATCH_SIZE,\n",
    "\t\tepochs=EPOCHS,\n",
    "\t\tshuffle=True,\n",
    "\t)\n",
    "\ttime_log(\"End Model Train\")\n",
    "\ttime_log(\"Start Model Test\")\n",
    "\tTP,FP,TN,FN,Sens,Spec,Acc,MCC,AUC = model_test(model, x_test, y_test)\n",
    "\twrite_data.append(TP)\n",
    "\twrite_data.append(FP)\n",
    "\twrite_data.append(TN)\n",
    "\twrite_data.append(FN)\n",
    "\twrite_data.append(Sens)\n",
    "\twrite_data.append(Spec)\n",
    "\twrite_data.append(Acc)\n",
    "\twrite_data.append(MCC)\n",
    "\twrite_data.append(AUC)\n",
    "    \n",
    "\ttime_log(\"End Model Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92db651-4fb4-472c-a404-ff76b95ae0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(write_data,a):\n",
    "    import csv\n",
    "    b=datetime.datetime.now()\n",
    "    write_data.append(b-a)\n",
    "    open_csv=open(\"imbalance_real.csv\",\"a\")\n",
    "    write_csv=csv.writer(open_csv)\n",
    "    write_csv.writerow(write_data)\n",
    "save_csv(write_data,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b29c2-5387-4dba-bf47-f611a78472e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
